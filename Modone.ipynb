{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Modone.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO0eizcqydtMGxwB+tpBrgy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrandolph95/STC510/blob/main/Modone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The FBI is tracking on a potential smuggling ring that is led by a shady character known by his nom de guerre of The Hamburgler and is using social media platforms to help organize her or his efforts. Your mission, should you choose to accept it, is to create a system that uses the API of various services to trace comments made over the last 72 hours that make mention of the terms that he is using as cover: cheese (payments), pickles (firearms), buns (identity covers), meat (targets), and sesame (keys). We need your help tracking this person and associates who may use these terms on social media."
      ],
      "metadata": {
        "id": "tgiyvvRzRZtr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a Python script that draws data from a subreddit (or from multiple subreddits, or from all subreddits) and stores it as a CSV.\n",
        "Think about the kinds of questions you might be able to address using data from Reddit. I was looking at /r/Phoenix because I wanted to extract place-based comments and learn from them. Here you are looking for particular keywords, within a particular timeframe. Design an extractor that does this, and also saves the dates and times of these comments to a human-readable version. It should also only collect the last 72 hours-worth of data."
      ],
      "metadata": {
        "id": "VwnsihmYQmNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praw\n",
        "!pip install pytz # suggested to use because kept getting tzinfo error when importing datetime. this will specify pacific timezone\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82VL9LfESbhO",
        "outputId": "8f1e8625-08a8-4043-b136-b1dd225b3b56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: praw in /usr/local/lib/python3.7/dist-packages (7.5.0)\n",
            "Requirement already satisfied: prawcore<3,>=2.1 in /usr/local/lib/python3.7/dist-packages (from praw) (2.3.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.7/dist-packages (from praw) (1.2.3)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.7/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from prawcore<3,>=2.1->praw) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.24.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (2018.9)\n",
            "Collecting datetime\n",
            "  Downloading DateTime-4.3-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[K     |████████████████████████████████| 60 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from datetime) (2018.9)\n",
            "Collecting zope.interface\n",
            "  Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB)\n",
            "\u001b[K     |████████████████████████████████| 251 kB 14.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zope.interface->datetime) (57.4.0)\n",
            "Installing collected packages: zope.interface, datetime\n",
            "Successfully installed datetime-4.3 zope.interface-5.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import time\n",
        "from time import sleep # importing to slow down execution\n",
        "import datetime \n",
        "from datetime import datetime\n",
        "from datetime import timedelta \n",
        "from praw.reddit import Submission # colab suggested this\n",
        "from praw.models.reddit.comment import Comment # colab suggested this"
      ],
      "metadata": {
        "id": "eCjIj24fSmjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. enter login and script info for reddit"
      ],
      "metadata": {
        "id": "5dbsqO8EQ3gH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "usr_name = \"nunya_9911\"\n",
        "usr_password = \"disposablepassword123!\"\n",
        "reddit_app_id = '0Fss0e88a5UL1dWmgk2vug'\n",
        "reddit_app_secret = 'AmCxyt0gEFlMe6r2TDs6ILzQfZI5Eg'\n",
        "reddit = praw.Reddit(user_agent=\"Mod 1 (by u/nunya_9911)\",\n",
        "                     client_id=reddit_app_id, client_secret=reddit_app_secret,\n",
        "                     # added the check for async as colab suggested I do so.\n",
        "                     username=usr_name, password=usr_password,check_for_async=False) "
      ],
      "metadata": {
        "id": "uKn5K4QUTYLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defines which subreddit we will be looking in. No preference so to subreddit so put 'all'\n",
        "subreddit = reddit.subreddit('all') "
      ],
      "metadata": {
        "id": "vtEfrmaKZ4qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This is an explanation of how to use search.** It is copied from the floating comment when I began to type \"search\". I put it here so it was easy to reference while I was typing everything out.\n",
        "\n",
        "def search(query: str, sort: str='relevance', syntax: str='lucene', time_filter: str='all', **generator_kwargs: Any) ->Iterator['praw.models.Submission']\n",
        "\n",
        "Return a .ListingGenerator for items that match query.\n",
        "\n",
        ":param query: The query string to search for.\n",
        ":param sort: Can be one of: relevance, hot, top, new, comments. (default:\n",
        "    relevance).\n",
        ":param syntax: Can be one of: cloudsearch, lucene, plain (default: lucene).\n",
        ":param time_filter: Can be one of: all, day, hour, month, week, year (default:\n",
        "    all).\n",
        "\n",
        "For more information on building a search query see:\n",
        "https://www.reddit.com/wiki/search\n",
        "\n",
        "For example, to search all subreddits for praw try:"
      ],
      "metadata": {
        "id": "H8bSnf-aikAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this labels the current date so I can use it later\n",
        "rightnow = datetime.now()\n",
        "# per the reading about time. the reading said to enter datetime.datetime.timedelta, but that didn't work.\n",
        "# did this instead. not sure if Python updated since the reading was published?\n",
        "delta = timedelta(hours=72)\n",
        "# this defines the time 3 days ago and converts it from datetime.datetime to a float\n",
        "the_last_seventytwo_hours = datetime.timestamp(rightnow - delta)\n",
        "\n"
      ],
      "metadata": {
        "id": "ymibjq_sa03N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"moduleone_missionhamburgler.csv\", 'w') as subfile:\n",
        "\n",
        "  # creating an empty list so I can add things to it\n",
        "  list_of_found_codenames = [] \n",
        "\n",
        "  # so reddit doesn't get mad at me\n",
        "  sleep(2)\n",
        "\n",
        "  # in all subreddits, I am searching for submissions that includes the Hamburgler's cover names\n",
        "  for submission in subreddit.search(\"cheese, bun, meat, pickle, sesame\",'new','lucene','week'):\n",
        "    # while completing the for loop above, it is going to make sure the submissions were made in the last 3 days\n",
        "    if submission.created_utc >= the_last_seventytwo_hours:\n",
        "      sleep(4)\n",
        "      # adding the submissions from the for loop to the list\n",
        "      list_of_found_codenames.append(submission.id)\n",
        "      \n",
        "\n",
        "\n",
        "      sleep(4)\n",
        "\n",
        "# this for loop will help format the submissions I added to the list\n",
        "  for eachtopic in list_of_found_codenames:\n",
        "    submission = reddit.submission(eachtopic)\n",
        "\n",
        "    # making sure reddit doesn't get mad. better safe than sorry :)\n",
        "    sleep(4)\n",
        "\n",
        "    # this formats all the submissions so it is easy to read\n",
        "    format = '*' + eachtopic + '* \"'\n",
        "    format += submission.title + '\", written by '\n",
        "    format += submission.author.name + '. @ '\n",
        "    format += datetime.strftime(datetime.fromtimestamp(submission.created_utc), ' %A, %B %d, %Y') + '.\\n'\n",
        "\n",
        "    # add everything we formatted to the subfile\n",
        "    subfile.write(format)\n",
        "\n",
        "  # making sure that it pulls all the comments from reddit by bypassing the \"more\" button\n",
        "  submission.comments.replace_more(limit=None)\n",
        "  commentlist = submission.comments.list()\n",
        "\n",
        "  sleep(4)\n",
        "\n",
        "# opening the file again to add/append the found comments to list\n",
        "with open(\"moduleone_missionhamburgler.csv\", 'a') as subfile:\n",
        "\n",
        "  # same as what I did for the submissions. \n",
        "  for eachcomment in commentlist:\n",
        "    sleep(2)\n",
        "\n",
        "    format = str(eachcomment) + ','\n",
        "    format += eachcomment.body.replace('\\n', '/') + ','\n",
        "    format += submission.author.name + ','\n",
        "    format += datetime.strftime(datetime.fromtimestamp(submission.created_utc), ' %A, %B %d, %Y') + ','\n",
        "\n",
        "    subfile.write(format) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "      \n",
        "  "
      ],
      "metadata": {
        "id": "lSz--y8hpAk2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}