{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce4dec4e-74df-440d-8755-fef74218322b",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. Parse, clean, and organize the Jeopardy! question data file to train a Naive Bayesian classifier.\n",
    "\n",
    "Just as we have built a classifier above, your aim here is to make sense of the data presented and create a binary classifier (\"high value\" and \"low value,\" based on the points available for each) for questions. Despite the large number of questions, this is an extraordinarily difficult classification problem. Consider it as a human coder: how often could you tell those questions that are \"easy\" versus \"hard\"? The degree to which you are successful in this is largely based on your own contextual knowledge--indeed, you might be tempted to classify questions you know the answer to as \"easy\" and those you do not as \"hard.\" The computer doesn't know the answers to any of these.\n",
    "\n",
    "For that reason, do not be discouraged if your classifier does not perform well. This constitutes an especially difficult problem for a simple classifier to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "821e35ea-dc49-4d66-a5f7-3146e5b74b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbe3a637-4895-448a-aa98-0c20f73947b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd1a1b4d-db54-4df1-a463-8d7be2d4986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48d4fb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64996c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38f59243-26f4-4b73-a197-405818304823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/malirandolph/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da3986e7-ad69-4aa2-bde3-0cec0bdbe8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48635183-c077-4559-9275-3ab825a9e2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/malirandolph/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f503957d-8821-4e99-92db-b285b428e1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/malirandolph/Downloads/jeopardy.json', 'r', newline='\\r\\n') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dda9cfec-de6c-4f59-b81a-94bc9f2f6b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts json file to csv\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "98025e84-219c-4818-9fae-b2cb87078e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('modfive.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d4405a8c-de38-4697-b658-2d935ec00c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('modfive.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8ad77dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216930"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9915399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning the rounds a value\n",
    "df['round'] = df['round'].apply(lambda x: 1 if x==\"Jeopardy!\" else 0 if x==\"Double Jeopardy!\" else -1).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "78db376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"question\"] = df[\"question\"].replace(\",\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2fe3b776",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"question\"] = df[\"question\"].replace(\":\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b1b3c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"question\"] = df[\"question\"].replace(\";\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a54b34a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/df/z6m4t_kj47s4qtpl7pfk3fgw0000gn/T/ipykernel_70707/2485659254.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['question'] = df['question'].str.replace('\\d+', '')\n"
     ]
    }
   ],
   "source": [
    "df['question'] = df['question'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f832ed0",
   "metadata": {},
   "source": [
    "for loop https://stackoverflow.com/questions/49926897/how-can-i-know-the-type-of-a-pandas-dataframe-cell\n",
    "Effective Pandas (book) Matt Hoffman?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "beb0f25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['question'] = df['question'].replace(to_replace=r'^https?:\\/\\/.*[\\r\\n]*',value='',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "df663f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['question'] = df['question'].replace(to_replace=r\"^[href{4}][a-z0-9_\\a-z]*\",value='',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "04b68aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['question'] = df['question'].replace('\"', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "caadfa69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/df/z6m4t_kj47s4qtpl7pfk3fgw0000gn/T/ipykernel_70707/3255980593.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['question'] = df['question'].str.replace(r'[^\\w\\s]+', '')\n"
     ]
    }
   ],
   "source": [
    "# removes all punctuation from dataframe\n",
    "df['question'] = df['question'].str.replace(r'[^\\w\\s]+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "192985e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['round', 'question']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6d0e8819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>air_date</th>\n",
       "      <th>question</th>\n",
       "      <th>value</th>\n",
       "      <th>answer</th>\n",
       "      <th>round</th>\n",
       "      <th>show_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HISTORY</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>For the last  years of his life Galileo was un...</td>\n",
       "      <td>$200</td>\n",
       "      <td>Copernicus</td>\n",
       "      <td>1</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>No   Olympian football star at Carlisle Indian...</td>\n",
       "      <td>$200</td>\n",
       "      <td>Jim Thorpe</td>\n",
       "      <td>1</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>The city of Yuma in this state has a record av...</td>\n",
       "      <td>$200</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>1</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>In  live on The Art Linkletter Show this compa...</td>\n",
       "      <td>$200</td>\n",
       "      <td>McDonald\\'s</td>\n",
       "      <td>1</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Signer of the Dec of Indep framer of the Const...</td>\n",
       "      <td>$200</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>1</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3-LETTER WORDS</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>In the title of an Aesop fable this insect sha...</td>\n",
       "      <td>$200</td>\n",
       "      <td>the ant</td>\n",
       "      <td>1</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HISTORY</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Built in  BC to link Rome  the South of Italy ...</td>\n",
       "      <td>$400</td>\n",
       "      <td>the Appian Way</td>\n",
       "      <td>1</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>No   steals for the Birmingham Barons  steals ...</td>\n",
       "      <td>$400</td>\n",
       "      <td>Michael Jordan</td>\n",
       "      <td>1</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>In the winter of  a record  inches of snow fel...</td>\n",
       "      <td>$400</td>\n",
       "      <td>Washington</td>\n",
       "      <td>1</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>This housewares store was named for the packag...</td>\n",
       "      <td>$400</td>\n",
       "      <td>Crate &amp; Barrel</td>\n",
       "      <td>1</td>\n",
       "      <td>4680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          category    air_date  \\\n",
       "0                          HISTORY  2004-12-31   \n",
       "1  ESPN's TOP 10 ALL-TIME ATHLETES  2004-12-31   \n",
       "2      EVERYBODY TALKS ABOUT IT...  2004-12-31   \n",
       "3                 THE COMPANY LINE  2004-12-31   \n",
       "4              EPITAPHS & TRIBUTES  2004-12-31   \n",
       "5                   3-LETTER WORDS  2004-12-31   \n",
       "6                          HISTORY  2004-12-31   \n",
       "7  ESPN's TOP 10 ALL-TIME ATHLETES  2004-12-31   \n",
       "8      EVERYBODY TALKS ABOUT IT...  2004-12-31   \n",
       "9                 THE COMPANY LINE  2004-12-31   \n",
       "\n",
       "                                            question value          answer  \\\n",
       "0  For the last  years of his life Galileo was un...  $200      Copernicus   \n",
       "1  No   Olympian football star at Carlisle Indian...  $200      Jim Thorpe   \n",
       "2  The city of Yuma in this state has a record av...  $200         Arizona   \n",
       "3  In  live on The Art Linkletter Show this compa...  $200     McDonald\\'s   \n",
       "4  Signer of the Dec of Indep framer of the Const...  $200      John Adams   \n",
       "5  In the title of an Aesop fable this insect sha...  $200         the ant   \n",
       "6  Built in  BC to link Rome  the South of Italy ...  $400  the Appian Way   \n",
       "7  No   steals for the Birmingham Barons  steals ...  $400  Michael Jordan   \n",
       "8  In the winter of  a record  inches of snow fel...  $400      Washington   \n",
       "9  This housewares store was named for the packag...  $400  Crate & Barrel   \n",
       "\n",
       "  round  show_number  \n",
       "0     1         4680  \n",
       "1     1         4680  \n",
       "2     1         4680  \n",
       "3     1         4680  \n",
       "4     1         4680  \n",
       "5     1         4680  \n",
       "6     1         4680  \n",
       "7     1         4680  \n",
       "8     1         4680  \n",
       "9     1         4680  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5e772a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_text'] = df['question'].apply(word_tokenize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0860fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_text'] = df['tokenized_text'].map(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c48f6e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f95ee8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectors = tfidfvectorizer.fit_transform(df['tokenized_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "605e6599",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfidf = tfidfvectorizer.transform(df['round'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "54f55832",
   "metadata": {},
   "outputs": [],
   "source": [
    "as_array = tfidf_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c4450cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b265dbf",
   "metadata": {},
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(lowercase=False, use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8477c9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes.fit(tfidf_vectors, df['round'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c82996d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = naive_bayes.predict(test_tfidf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
