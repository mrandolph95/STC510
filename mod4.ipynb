{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f98e6abd-7eba-48ea-bb68-77e3f3d3def2",
   "metadata": {},
   "source": [
    "RE: OSINT Monitoring\n",
    "\n",
    "Dear Data Consultant,\n",
    "\n",
    "Your assistance thus far has helped us in our investigations. I'm afraid we need your help once again. The Hamburgler has been captured, but her associates are now attempting to fence explosives and drugs in places like Craigslist garage sales. They seem to be using terms like \"mattress,\" \"cabinet,\" and \"wrench,\" though there may be others. We need a tool that we can schedule to check daily for these keywords showing up in local markets so we can then investigate them more fully.\n",
    "\n",
    "We know about Craigslist, but we suspect they are also using Parler and Gab--not just to sell goods but also to coordinate events and make connections. We aren't sure which terms they are using there, so you'll need to use your best judgment .\n",
    "\n",
    "- Smith, Agent-in-Charge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964ed2a6-b365-4522-ad24-4b9a56074f64",
   "metadata": {},
   "source": [
    "Write a Python script that, when executed once, goes to the Phoenix Craigslist site, and crawls each of the listings in the \"Garage & Moving Sales\" page, looking for mentions of the words above, and generates a csv that lists the keyword and the URL at which it was found. \n",
    "Pass the Web Scraping Basics quiz with a score of 85% or better.\n",
    "This project is a bit more complicated than what we've done in the tutorials, but only a little bit. It will require you to extract the links (found in \"a\" tags) and sort through them to make sure you are only getting the ones in the gallery. You may need to look through the BeautifulSoup documentation to figure out how to get at the hrefs from those A tags. Note that Craigslist seems to be blocking Google IPs, so you'll need to do this one from your own machine. Once you have a list of URLs, you'll need to crawl them one-by-one, and search the text of the ads for each of your keywords (see above). Note that while Craigslist has an API, you need to be scraping directly here to show your scraping skills. Remember to build in a sizeable \"sleep\" between each request. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "057d65e7-2ee0-45f3-9d26-c336b7d94559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /opt/anaconda3/lib/python3.9/site-packages (4.1.0)\n",
      "Requirement already satisfied: urllib3[secure]~=1.26 in /opt/anaconda3/lib/python3.9/site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /opt/anaconda3/lib/python3.9/site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in /opt/anaconda3/lib/python3.9/site-packages (from selenium) (0.19.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in /opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: outcome in /opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in /opt/anaconda3/lib/python3.9/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/lib/python3.9/site-packages (from trio-websocket~=0.9->selenium) (1.0.0)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in /opt/anaconda3/lib/python3.9/site-packages (from urllib3[secure]~=1.26->selenium) (21.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in /opt/anaconda3/lib/python3.9/site-packages (from urllib3[secure]~=1.26->selenium) (3.4.8)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.9/site-packages (from urllib3[secure]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/lib/python3.9/site-packages (from cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (2.20)\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/anaconda3/lib/python3.9/site-packages (from pyOpenSSL>=0.14->urllib3[secure]~=1.26->selenium) (1.16.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /opt/anaconda3/lib/python3.9/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1c386e4-178f-4c96-b92d-cd38c4eb5be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.9/site-packages (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4) (2.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "b85e4937-47f8-4ede-89ce-7667697c5f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import csv\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "# this imports the path to get to selenium from a separate\n",
    "# python file\n",
    "from infos import PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "b4af233e-c1cc-441a-9383-bc1b7ef5e544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hamburgler():\n",
    "    \n",
    "    # this opens Chrome from the kernel\n",
    "    driver = webdriver.Chrome(executable_path=PATH)\n",
    "    \n",
    "    sleep(2)\n",
    "    \n",
    "    # this takes me to craigslist\n",
    "    craigslist = driver.get('https://phoenix.craigslist.org/')\n",
    "    \n",
    "    sleep(4)\n",
    "    \n",
    "    # this clicks on the garage and home sales hyperlink\n",
    "    garagesales = driver.find_element_by_class_name('gms').click()\n",
    "    \n",
    "    sleep(4)\n",
    "    \n",
    "    #this uses the send keys function to search all the listings to find ones with the word \"mattress\"\n",
    "    driver.find_element_by_id('query').send_keys('mattress',Keys.RETURN)\n",
    "    \n",
    "    sleep(2)\n",
    "    \n",
    "    # this grabs the page info so Beautiful soup can be used to\n",
    "    # find specific tags\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    \n",
    "    sleep(2)\n",
    "    \n",
    "    # this narrows down the tags so I can get to href. This was done this way\n",
    "    # because I didn't want to grab ALL the links on the pages, just the \n",
    "    # ones that had the relevant search results.\n",
    "    mattress = soup.find('ul', {'id': 'search-results', 'class': 'rows'})\n",
    "    listings_mattress = mattress.find_all('li', 'result-row')\n",
    "    \n",
    "    sleep(2)\n",
    "    \n",
    "    # this grabs the relevant url under the href tags\n",
    "    # and then labels the url accordingly\n",
    "    for listings_m in listings_mattress:\n",
    "        link_m = listings_m.a['href']\n",
    "        link_m += 'n/n/' + ' - mattress'\n",
    "        \n",
    "        # this prints a status update\n",
    "        print(f'mattress search results: {link} ')\n",
    "        \n",
    "    # this clears the search box...    \n",
    "    driver.find_element_by_id('query').clear()\n",
    "    \n",
    "    sleep(2)\n",
    "    \n",
    "    # so the function can search for the next code word\n",
    "    driver.find_element_by_id('query').send_keys('cabinet', Keys.RETURN)\n",
    "    \n",
    "    sleep(3)\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    \n",
    "    cabinet = soup.find('ul', {'id': 'search-results', 'class': 'rows'})\n",
    "    listings_cabinet = cabinet.find_all('li', 'result-row')\n",
    "    \n",
    "    for listings_c in listings_cabinet:\n",
    "        link_c = listings_c.a['href']\n",
    "    \n",
    "        link_c += '/n/n' + ' - cabinet'\n",
    "    \n",
    "        print(f'cabinet search results: {link_c} ')\n",
    "        \n",
    "        sleep(2)\n",
    "    \n",
    "    driver.find_element_by_id('query').clear()\n",
    "    \n",
    "    sleep(2)\n",
    "    \n",
    "    driver.find_element_by_id('query').send_keys('wrench', Keys.RETURN)\n",
    "    \n",
    "    sleep(3)\n",
    "    \n",
    "    wrench = soup.find('ul', {'id': 'search-results', 'class': 'rows'})\n",
    "    \n",
    "    listings_wrench = wrench.find_all('li', 'result-row')\n",
    "    \n",
    "    for listings_w in listings_wrench:\n",
    "        link_w = listings_w.a['href']\n",
    "    \n",
    "        link_w += '/n/n' + ' - wrench'\n",
    "        print(f'wrench search results: {link_w} ')\n",
    "     \n",
    "    # creating an empty list so the search results can be added\n",
    "    allposts = []\n",
    "    \n",
    "    # appended a list of the search results to the empty list\n",
    "    allposts.append([link_w, link_m, link_c])\n",
    "    \n",
    "    # creating a data fram using pandas\n",
    "    df = pd.DataFrame(allposts)\n",
    "    \n",
    "    # creats a new csv and adds the information from the list\n",
    "    df.to_csv('modfour.csv')\n",
    "    \n",
    "    # ends function and closes webdriver\n",
    "    return;\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "28d9b540-028e-49aa-8dd8-ac122ac8d35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting schedule\n",
      "  Downloading schedule-1.1.0-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: schedule\n",
      "Successfully installed schedule-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "10387cc6-a90b-4333-b134-21e3166630a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import schedule\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5affcd0-5304-4b3c-ab79-c04f2661de30",
   "metadata": {},
   "source": [
    "schedule documentation link for future reference: https://schedule.readthedocs.io/en/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "a4e2f824-5e76-4992-9227-11f4b53416a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/df/z6m4t_kj47s4qtpl7pfk3fgw0000gn/T/ipykernel_28055/2850852573.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mschedule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pending\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# this schedules the find_hamblurgler function to run every day at 1:00 p.m.\n",
    "\n",
    "def job():\n",
    "    print(\"I'm working...\")\n",
    "    \n",
    "schedule.every().day.at(\"13:00\").do(find_hamburgler)\n",
    "\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac8799c-3f22-44ae-98a5-4c35ae4d6e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
